{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unidecode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munidecode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unidecode\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'unidecode'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html\n",
    "import time\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import crawler\n",
    "import models\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49\"}\n",
    "\n",
    "def ObtainAllUrls():\n",
    "    marcas = sorted([\n",
    "        'mazda',\n",
    "        'mercedes-benz','nissan','bmw',\n",
    "        'chevrolet','toyota','renault','audi',\n",
    "        'volkswagen','honda','kia','ford'])\n",
    "    # marcas = sorted([\n",
    "    # 'subaru','mazda','mahindra','mini','dodge','citroen','mitsubishi','hyundai','ssangyong','mg',\n",
    "    # 'mercedes-benz','nissan','bmw','chevrolet','toyota','jeep','ds','dfm/dfsk','unico-dueno-nissan',\n",
    "    # 'brilliance','mercedes-benz-blindada','renault','audi','chery','foton','zotye','seat','hafei',\n",
    "    # 'skoda','porsche','opel','changan','volkswagen','fiat','ram','suzuki','honda','great-wall','kia',\n",
    "    # 'jac','cupra','volvo','jaguar','peugeot','land-rover','hummer','daihatsu','byd','ford'])\n",
    "\n",
    "    dim_ubicacion = pd.read_csv(\"dim_ubicacion_all.csv\", sep=\"|\")\n",
    "    dim_ubicacion = dim_ubicacion.sample(3)\n",
    "    #dim_ubicacion = dim_ubicacion[dim_ubicacion[\"ciudad\"] == \"cali\"]\n",
    "    oferts_full = []\n",
    "\n",
    "    departamentos = dim_ubicacion[\"departamento\"].unique().tolist()\n",
    "    for departamento in tqdm(departamentos, desc=\"Parseando ofertas de paginación\"):\n",
    "        #print(departamento)\n",
    "\n",
    "        ciudades = dim_ubicacion[dim_ubicacion[\"departamento\"] == departamento][\"ciudad\"].unique().tolist()\n",
    "        for ciudad in ciudades:\n",
    "            #print(ciudad)\n",
    "            \n",
    "            for marca in marcas:\n",
    "                size = 0 # validacion\n",
    "                for pag in range(49, 1969, 48):  # 48*n | El máximo de paginación por categoría es de 42, 1969 vehículos\n",
    "                    url = f\"https://carros.mercadolibre.com.co/{marca}/{departamento}/{ciudad}/_Desde_{pag}_ITEM*CONDITION_2230581_NoIndex_True\"\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    content = response.content.decode('utf-8')\n",
    "                    href_ofert = '//a[contains(@class, \"poly-component__title\")]'\n",
    "                    \n",
    "                    if \"Escribe en el buscador lo que quieres encontrar.\" not in content:\n",
    "                        tree = html.fromstring(content)\n",
    "                        urls_ofert_temp = [x.get('href') for x in tree.xpath(href_ofert)]\n",
    "                        oferts_full.append(urls_ofert_temp)\n",
    "                        time.sleep(1)\n",
    "                        size += len(urls_ofert_temp)\n",
    "                        #print(url)\n",
    "                    else:\n",
    "                        base_url = \"/\".join(url.split(\"/\")[:6])\n",
    "                        response = requests.get(base_url, headers=headers)\n",
    "                        tree = html.fromstring(response.content)\n",
    "                        urls_ofert_temp = [x.get('href') for x in tree.xpath(href_ofert)]\n",
    "                        oferts_full.append(urls_ofert_temp)\n",
    "                        time.sleep(1)\n",
    "                        size += len(urls_ofert_temp)\n",
    "                        #print(base_url)\n",
    "                        \n",
    "                        break\n",
    "                    \n",
    "                #print(marca, size)\n",
    "\n",
    "    oferts_full = [i for sublist in oferts_full for i in sublist]\n",
    "    oferts_full = list(set(oferts_full))\n",
    "    pd.DataFrame({\"url\":oferts_full}).to_parquet(\"urls.parquet\", compression = \"gzip\")\n",
    "    print(\"Total de ofertas:\", len(oferts_full))\n",
    "\n",
    "    return oferts_full\n",
    "\n",
    "\n",
    "def ReadAllUrls():\n",
    "    oferts_full = pd.read_parquet(\"urls.parquet\")[\"url\"].tolist()\n",
    "    oferts_full = [i for sublist in oferts_full for i in sublist]\n",
    "    oferts_full = list(set(oferts_full))\n",
    "    print(len(oferts_full))\n",
    "\n",
    "\n",
    "def ScrapUrl(oferts_full, \n",
    "             delay = 1,\n",
    "             counter_file = 200,\n",
    "             repetead = False\n",
    "             ):\n",
    "    df = pd.DataFrame()\n",
    "    error_server = 0\n",
    "    counter = 1\n",
    "    record_count = 0\n",
    "    repetead = \"_R\" if repetead == True else \"C\" # C: registros correcto, R: registros que se volvieron a ingestar debido a error en andes-table\n",
    "\n",
    "    actual_date = datetime.now()\n",
    "    month = str(actual_date.month)\n",
    "    month = np.where(len(month) == 1, \"0\" + month, month)\n",
    "    day = str(actual_date.day)\n",
    "    day = np.where(len(day) == 1, \"0\" + day, day)\n",
    "    actual_date = str(actual_date.year) +  str(month) + str(day)\n",
    "\n",
    "    for oferta_temp in tqdm(list(oferts_full), desc=\"Obteniendo variables\"):\n",
    "        df_oferta_temp, error_server_temp = crawler.GetCarAtributes(oferta_temp, debug = \"OFF\")\n",
    "        df_oferta_temp = df_oferta_temp[df_oferta_temp[\"estado\"] == 1]\n",
    "        df = pd.concat([df, df_oferta_temp], axis = 0)\n",
    "\n",
    "        # if \"marca\" in df_oferta_temp.columns:\n",
    "        #     print(\"correcto\")\n",
    "        # else:\n",
    "        #     print(\"incorrecto\")\n",
    "\n",
    "        error_server += error_server_temp\n",
    "        if error_server > len(oferts_full)*0.3:\n",
    "            print(\"WARNING: Error Server > 30% registers\")\n",
    "            break\n",
    "\n",
    "        \n",
    "        #if \"marca\" in df_oferta_temp:\n",
    "\n",
    "        record_count += 1\n",
    "        if record_count == counter_file:\n",
    "            df.to_csv(f\"temp/{actual_date}_{str(counter) + repetead}.csv\", sep=\"|\", index=False)\n",
    "            df = pd.DataFrame()\n",
    "            counter += 1\n",
    "            record_count = 0  \n",
    "            time.sleep(15)\n",
    "\n",
    "        time.sleep(delay)\n",
    "    print(\"Oferts Error Server 403:\", error_server)\n",
    "    print(\"Dimensión df:\", df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def ReadTemp():\n",
    "    files = os.listdir(\"temp\")\n",
    "    files = [pd.read_csv(\"temp/\" + x, sep = \"|\") for x in files]\n",
    "    df = pd.concat(files, axis = 0)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    print(\"Dimension ./temp:\", df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def TrainModel(df):\n",
    "    cols_numeric = [\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\"]\n",
    "    for col in cols_numeric:\n",
    "        df[col] = df[col].astype(float)\n",
    "    df_train = df[[\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\", \"transmision\", \"tipo_de_combustible\", \"tipo_de_carroceria\", \"puertas\",\n",
    "                   \"marca\", \"modelo_agrup\"]].dropna()\n",
    "    df_train[\"precio\"] = np.log(df_train[\"precio\"].astype(float))\n",
    "\n",
    "    X_train, X_test, y_train, y_test = models.TrainTestDummies(df = df_train, y='precio',\n",
    "                                                                 dummies = [\"marca\", \"modelo_agrup\", \"transmision\", \"tipo_de_combustible\", \n",
    "                                                                            \"tipo_de_carroceria\", \"puertas\"])\n",
    "    print(\"Dimensión train:\", X_train.shape)\n",
    "    print(\"Dimensión test:\", X_test.shape)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators': [380],\n",
    "        'max_depth': [5],\n",
    "        'learning_rate': [0.2],\n",
    "        'min_child_weight': [5],\n",
    "        'booster': ['gbtree'],\n",
    "        'eta': [0.001],\n",
    "        'gamma': [0.001],\n",
    "        'lambda': [0.99],\n",
    "        'alpha': [0.99],\n",
    "        'subsample': [1],\n",
    "        'colsample_bytree': [0.5],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "    model = models.XGBoost(X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test, \n",
    "                        param_grid = param_grid, cv = 2).fit()\n",
    "\n",
    "    models.SaveModel(model, \"xgboost.pkl\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "   ---------------------------------------- 0.0/235.5 kB ? eta -:--:--\n",
      "   -------------------------------------- - 225.3/235.5 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 235.5/235.5 kB 2.9 MB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\jose\\downloads\\wpy64-31241\\python-3.12.4.amd64\\lib\\site-packages (1.26.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install unidecode\n",
    "!pip install matplotlib \n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Iniciando Web Scrapping\")\n",
    "counter_file = 100\n",
    "delay = 1\n",
    "\n",
    "time1 = datetime.now()\n",
    "\n",
    "#func.ClearFolder(\"temp\")\n",
    "\n",
    "# Obtener número actual de ofertas\n",
    "#primer_pagina = \"https://carros.mercadolibre.com.co/valle-del-cauca/cali/_Desde_49_ITEM*CONDITION_2230581_NoIndex_True\"\n",
    "primer_pagina = \"https://carros.mercadolibre.com.co/_Desde_49_ITEM*CONDITION_2230581_NoIndex_True\" # Nacional\n",
    "response = requests.get(primer_pagina, headers=headers)\n",
    "response.raise_for_status()\n",
    "tree = html.fromstring(response.content)\n",
    "num_ofertas = int(tree.xpath('//*[@id=\"root-app\"]/div/div[3]/aside/div[2]/span')[0].text_content().strip().replace(\".\",\"\").replace(\" resultados\",\"\"))\n",
    "print(\"Total ofertas:\", num_ofertas)\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 1: obtener la url de cada oferta\")\n",
    "oferts_urls = ObtainAllUrls()\n",
    "#oferts_urls = pd.Series(oferts_urls).sample(200).tolist()\n",
    "print(\"=\"*120)\n",
    "\n",
    "oferts_urls = pd.Series(oferts_urls).sample(3000).tolist()\n",
    "\n",
    "\n",
    "print(\"Bloque 2: obtener los atributos de cada oferta\")\n",
    "df = ScrapUrl(oferts_urls, delay = delay, counter_file = counter_file)\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 3: volver a consumir los registros con errores\")\n",
    "df = ReadTemp()\n",
    "urls_complete = df[df[\"estado\"] == 1][\"url\"]\n",
    "pendient = set(oferts_urls) - set(urls_complete)\n",
    "print(\"Registros correctos:\", len(urls_complete), round(len(urls_complete)/(len(urls_complete) + len(pendient))*100), 2)\n",
    "print(\"Registros con error:\", len(pendient), round(len(pendient)/(len(urls_complete) + len(pendient))*100), 2)\n",
    "df_error = ScrapUrl(pendient, delay = delay, counter_file = counter_file, repetead = True)\n",
    "\n",
    "\n",
    "print(\"Bloque 4: concatenado final de archivos\")\n",
    "df = ReadTemp()\n",
    "df = df[df[\"estado\"] == 1]\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 5: refinando atributos\")\n",
    "df = crawler.RefineAtributes(df)\n",
    "print(\"-\"*120)\n",
    "\n",
    "print(\"Correctos final:\", df[df[\"estado\"] == 1].shape[0], \"|\", round((df[df[\"estado\"] == 1].shape[0]/df.shape[0])*100, 2), \"%\")\n",
    "print(\"Dimensión df:\", df.shape)\n",
    "vars_duplicates = [\"marca\", \"modelo\", \"precio\", \"año\", \"motor\", \"km\", \"transmision\", \"tipo_de_carroceria\", \"tipo_de_combustible\", \n",
    "                   \"barrio\", \"ciudad\", \"departamento\"]\n",
    "print(\"Duplicados por atributos:\", df[df.duplicated(vars_duplicates)].shape[0])\n",
    "print(\"Duplicados por url:\", df[df.duplicated([\"url\"])].shape[0])\n",
    "print(\"Duplicados por id:\", df[df.duplicated([\"id_ofert\"])].shape[0])\n",
    "df = df.drop_duplicates(\"url\").drop_duplicates(\"id_ofert\").drop_duplicates(vars_duplicates)\n",
    "print(\"Dimensión df sin duplicados:\", df.shape)\n",
    "print(\"-\"*120)\n",
    "pendient = set(oferts_urls) - set(df[\"url\"])\n",
    "print(\"Final: Registros correctos:\", len(urls_complete))\n",
    "print(\"Final: Registros con error (excluidos):\", len(pendient))\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 6: imputación de datos faltantes en categoricas\"); print(\"-\"*50)\n",
    "df = models.ImputCategorics(df, \"marca_modelo\", \"transmision\", 70)\n",
    "df = models.ImputCategorics(df, \"marca_modelo\", \"tipo_de_carroceria\", 70)\n",
    "df = models.ImputCategorics(df, \"marca_modelo\", \"tipo_de_combustible\", 70)\n",
    "df = models.ImputCategorics(df, \"marca_modelo\", \"puertas\", 70)\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 7: criterios de exclusión e inclusión\"); print(\"-\"*50)\n",
    "dim = df.shape[0]\n",
    "print(\"Registros:\", dim)\n",
    "print(\"Excluyendo...\")\n",
    "umbral_precio = np.percentile(df[\"precio\"], 97)\n",
    "df = df[(df[\"precio\"] > 0) & (df[\"precio\"] <= umbral_precio)]\n",
    "print(f\"Precio != 0 & < {round(umbral_precio,2)}:\", df.shape)\n",
    "df = df[df[\"año\"] != int(datetime.today().year)]\n",
    "print(\"Año != actual:\", df.shape)\n",
    "df = df[df[\"km\"] != 0]\n",
    "print(\"Km > 0:\", df.shape)\n",
    "df = df[(df[\"motor\"] > 0.7) | (df[\"motor\"].isnull())]\n",
    "print(\"Motor > 0.7 | null\", df.shape)\n",
    "df = df[df[\"antiguedad\"] > 0]\n",
    "print(\"Antigúedad > 0:\", df.shape)\n",
    "df = df[~df[\"tipo_de_carroceria\"].isin([\"Eléctrico\", 'Light Truck','Monovolumen', 'ESTACAS'])]\n",
    "print(\"Carrocerias:\", df.shape)\n",
    "df = df[df[\"transmision\"].notna()]\n",
    "print(\"transmision notna:\", df.shape[0])\n",
    "df = df[df[\"tipo_de_combustible\"].notna()]\n",
    "print(\"Combustible notna:\", df.shape[0])\n",
    "df = df[df[\"tipo_de_carroceria\"].notna()]\n",
    "print(\"Carroceria notna:\", df.shape[0])\n",
    "print(\"*Total eliminados:\", dim - df.shape[0], f\"({round(((dim - df.shape[0]) / dim)*100, 2)}%)\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 8: imputando valores faltantes en cilindraje\"); print(\"-\"*50)\n",
    "df[\"motor\"] = np.log(df[\"motor\"])\n",
    "df = models.ImputRegression(df, y = 'motor', x = ['año', 'marca', 'modelo', 'tipo_de_combustible', 'transmision', 'tipo_de_carroceria', \n",
    "                                                  'puertas'], method = \"xgboost\")\n",
    "df[\"motor\"] = round(np.exp(df[\"motor\"]), 2)\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"---------- Precio ----------\")\n",
    "print(\"Media:\", round(df[\"precio\"].mean(), 2))\n",
    "print(\"Mediana:\", round(df[\"precio\"].median(), 2))\n",
    "print(\"CV:\", round(np.sqrt(df[\"precio\"].var()) / df[\"precio\"].mean() * 100, 2), \"%\")\n",
    "print(\"Umbral:\", round(umbral_precio, 3))\n",
    "print(\"=\"*120)\n",
    "\n",
    "\n",
    "print(\"Bloque 9: agrupacion de variables\"); print(\"-\"*50)\n",
    "# df = models.ClassicationTreeGroup(df, y = \"precio\", x = \"marca\", max_leaf_nodes=[4, 5, 6]); print(\"-\"*50)\n",
    "df = models.ClassicationTreeGroup(df, y = \"precio\", x = \"modelo\", max_leaf_nodes=[30])\n",
    "print(\"=\"*120)\n",
    "\n",
    "df = df[['estado', 'nombre', 'precio', 'año', 'km', 'km_por_año', 'barrio', 'ciudad','departamento', 'fecha', 'url', 'dtm_etl', \n",
    "         'marca_modelo', 'marca', \n",
    "         'modelo', 'modelo_agrup', 'version','color', 'tipo_de_combustible', 'puertas', 'transmision', 'motor','tipo_de_carroceria', \n",
    "         'ultimo_digito_de_la_placa','con_camara_de_reversa']]\n",
    "\n",
    "df.to_csv(\"df_refine.csv\", sep = \"|\", index = False)\n",
    "\n",
    "\n",
    "print(\"Bloque 10: entrenando modelo predictivo\"); print(\"-\"*50)\n",
    "model = TrainModel(df)\n",
    "\n",
    "print(\"Dimensión final:\", df.shape)\n",
    "print(f\"El proceso tardó: {datetime.now() - time1}\")\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_numeric = [\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\"]\n",
    "for col in cols_numeric:\n",
    "    df[col] = df[col].astype(float)\n",
    "df_train = df[[\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\", \"transmision\", \"tipo_de_combustible\", \"tipo_de_carroceria\", \"puertas\",\n",
    "                \"marca\", \"modelo_agrup\"]].dropna()\n",
    "df_train[\"precio\"] = np.log(df_train[\"precio\"].astype(float))\n",
    "\n",
    "X_train, X_test, y_train, y_test = models.TrainTestDummies(df = df_train, y='precio',\n",
    "                                                                dummies = [\"marca\", \"modelo_agrup\", \"transmision\", \"tipo_de_combustible\", \n",
    "                                                                        \"tipo_de_carroceria\", \"puertas\"])\n",
    "print(\"Dimensión train:\", X_train.shape)\n",
    "print(\"Dimensión test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "año = 2005\n",
    "km = 150000\n",
    "motor = 4\n",
    "marca = \"Ford\"\n",
    "modelo = \"Explorer\"\n",
    "tipo_de_carroceria = \"Camioneta\"\n",
    "puertas = \"4_5\"\n",
    "transmision = \"Automático\"\n",
    "tipo_de_combustible = \"Gasolina\"\n",
    "modelo = df[df[\"modelo\"] == modelo][\"modelo_agrup\"].unique()[0]\n",
    "df_new = pd.DataFrame({\"año\":[año], \"km\":[km], \"motor\":[motor], \"km_por_año\":[km/(2024-año)], \n",
    "                       \"transmision\":[transmision], \"tipo_de_combustible\":[tipo_de_combustible],\n",
    "                       \"marca\":[marca], \"modelo_agrup\":[modelo], \"puertas\":[puertas], \n",
    "                       \"tipo_de_carroceria\":[tipo_de_carroceria]})\n",
    "df_new = pd.get_dummies(df_new, columns = [\"transmision\", \"tipo_de_combustible\", \"marca\", \"modelo_agrup\",\n",
    "                                           \"puertas\", \"tipo_de_carroceria\"]).replace(True, 1)\n",
    "\n",
    "cols = X_train.columns.tolist()\n",
    "for col in cols:\n",
    "    if col not in df_new.columns:\n",
    "        df_new[col] = 0\n",
    "df_new = df_new[X_train.columns.tolist()]\n",
    "np.exp(model.predict(df_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ubicacion = df[[\"ciudad\", \"departamento\"]].drop_duplicates().sort_values(\"departamento\").dropna()\n",
    "df_ubicacion[\"ciudad\"] = [unidecode(x.lower().rstrip(\" \").lstrip(\" \").replace(\" \", \"-\")) for x in df_ubicacion[\"ciudad\"].astype(str)]\n",
    "df_ubicacion[\"departamendf_ubicacion.to_csv(\"dim_ubicacion.csv\", sep = \"|\", index = False)to\"] = [unidecode(x.lower().rstrip(\" \").lstrip(\" \").replace(\" \", \"-\")) for x in df_ubicacion[\"departamento\"].astype(str)]\n",
    "df_ubicacion[\"departamento\"] = df_ubicacion[\"departamento\"].replace({\"bogota-d.c.\":\"bogota-dc\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ubicacion.to_csv(\"dim_ubicacion_full.csv\", sep = \"|\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_ubicacion[\"ciudad\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marcas = set([unidecode(x.lower()).replace(\" \", \"-\") for x in df[\"marca\"]])\n",
    "print(len(marcas))\n",
    "\"','\".join(marcas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
