{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html\n",
    "import time\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd().replace(\"\\\\\",\"/\") + \"/src\")\n",
    "import spider "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando Web Scrapping\n",
      "Total ofertas: 6637\n",
      "========================================================================================================================\n",
      "Bloque 1: obtener la url de cada oferta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parseando ofertas por paginación: 100%|██████████| 1/1 [04:24<00:00, 264.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ofertas obtenidas: 1755\n",
      "========================================================================================================================\n",
      "Bloque 2: obtener los atributos de cada oferta\n",
      "Ofertas a ingestar: 1755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Obteniendo variables:  13%|█▎        | 236/1755 [13:50<1:04:20,  2.54s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df, model = spider.Main(counter_file = 100, delay = 1, reprocess_oferts_urls = False) # Funcional 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(\"/workspaces/cars/src\")\n",
    "import models\n",
    "import functions as func\n",
    "import paths as ph\n",
    "import car.src.homologations as ho\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36 Edg/103.0.1264.49\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://carro.mercadolibre.com.co/MCO-2631967034-audi-a4-tfsi-atraccion-14-_JM#polycard_client=search-nordic&position=1&search_layout=grid&type=item&tracking_id=4a729cc7-5835-4be1-80b1-d94b04558187\"\n",
    "\n",
    "http_proxy  = \"http://10.10.1.10:3128\"\n",
    "https_proxy = \"https://10.10.1.11:1080\"\n",
    "ftp_proxy   = \"ftp://10.10.1.10:3128\"\n",
    "\n",
    "proxies = {\n",
    "  \"http\": \"http://10.10.1.10:3128\",\n",
    "  \"https\": \"https://10.10.1.10:1080\",\n",
    "}\n",
    "\n",
    "proxy = \"20.111.54.16:8123\"\n",
    "\n",
    "import random\n",
    "#proxy = random.choice(proxies)\n",
    "#proxy = {\"http\":\"http://\" + proxy, \"https\": \"https://\" + proxy}\n",
    "response = requests.get(url, headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "response.raise_for_status()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import concurrent.futures\n",
    "\n",
    "#get the list of free proxies\n",
    "def getProxies():\n",
    "    r = requests.get('https://free-proxy-list.net/')\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    table = soup.find('tbody')\n",
    "    proxies = []\n",
    "    for row in table:\n",
    "        if row.find_all('td')[4].text =='elite proxy':\n",
    "            proxy = ':'.join([row.find_all('td')[0].text, row.find_all('td')[1].text])\n",
    "            proxies.append(proxy)\n",
    "        else:\n",
    "            pass\n",
    "    return proxies\n",
    "\n",
    "def extract(proxy):\n",
    "    #this was for when we took a list into the function, without conc futures.\n",
    "    #proxy = random.choice(proxylist)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    try:\n",
    "        #change the url to https://httpbin.org/ip that doesnt block anything\n",
    "        r = requests.get('https://httpbin.org/ip', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=1)\n",
    "        print(r.json(), r.status_code)\n",
    "    except requests.ConnectionError as err:\n",
    "        print(repr(err))\n",
    "    return proxy\n",
    "\n",
    "proxylist = getProxies()\n",
    "#print(len(proxylist))\n",
    "\n",
    "#check them all with futures super quick\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(extract, proxylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(proxylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "import csv\n",
    "import concurrent.futures\n",
    "\n",
    "#opens a csv file of proxies and prints out the ones that work with the url in the extract function\n",
    "\n",
    "# proxylist = []\n",
    "\n",
    "# with open('proxylist.csv', 'r') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     for row in reader:\n",
    "#         proxylist.append(row[0])\n",
    "\n",
    "def extract(proxy):\n",
    "    #this was for when we took a list into the function, without conc futures.\n",
    "    #proxy = random.choice(proxylist)\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "    try:\n",
    "        #change the url to https://httpbin.org/ip that doesnt block anything\n",
    "        r = requests.get(\"https://carro.mercadolibre.com.co/MCO-2631967034-audi-a4-tfsi-atraccion-14-_JM#polycard_client=search-nordic&position=1&search_layout=grid&type=item&tracking_id=4a729cc7-5835-4be1-80b1-d94b04558187\", headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "        print(r.json(), ' | Works')\n",
    "        print(proxy)\n",
    "    except:\n",
    "        pass\n",
    "    return proxy\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        executor.map(extract, proxylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"20.111.54.16:8123\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "r = requests.get('https://httpbin.org/ip', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "print(r.json(), ' | Works')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proxy 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import traceback\n",
    "\n",
    "def get_free_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # request and grab content\n",
    "    soup = bs(requests.get(url).content, 'html.parser')\n",
    "    # to store proxies\n",
    "    proxies = []\n",
    "    for row in soup.find(\"table\", attrs={\"class\": \"table-striped\"}).find_all(\"tr\")[1:]:\n",
    "        tds = row.find_all(\"td\")\n",
    "        try:\n",
    "            ip = tds[0].text.strip()\n",
    "            port = tds[1].text.strip()\n",
    "            proxies.append(str(ip) + \":\" + str(port))\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return proxies\n",
    "\n",
    "url = \"http://httpbin.org/ip\"\n",
    "proxies = get_free_proxies()\n",
    "\n",
    "for i in range(len(proxies)):\n",
    "\n",
    "    #printing req number\n",
    "    print(\"Request Number : \" + str(i+1))\n",
    "    proxy = proxies[i]\n",
    "    try:\n",
    "        response = requests.get(url, proxies = {\"http\":proxy, \"https\":proxy})\n",
    "        print(response.json())\n",
    "    except:\n",
    "        # if the proxy Ip is pre occupied\n",
    "        print(\"Not Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxy = \"20.111.54.16:8123\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:80.0) Gecko/20100101 Firefox/80.0'}\n",
    "r = requests.get('https://httpbin.org/ip', headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "print(r.json(), ' | Works')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://carro.mercadolibre.com.co/MCO-2631967034-audi-a4-tfsi-atraccion-14-_JM#polycard_client=search-nordic&position=1&search_layout=grid&type=item&tracking_id=4a729cc7-5835-4be1-80b1-d94b04558187\"\n",
    "\n",
    "http_proxy  = \"http://10.10.1.10:3128\"\n",
    "https_proxy = \"https://10.10.1.11:1080\"\n",
    "ftp_proxy   = \"ftp://10.10.1.10:3128\"\n",
    "\n",
    "proxies = {\n",
    "  \"http\": \"http://10.10.1.10:3128\",\n",
    "  \"https\": \"https://10.10.1.10:1080\",\n",
    "}\n",
    "\n",
    "proxy = \"20.111.54.16:8123\"\n",
    "\n",
    "import random\n",
    "#proxy = random.choice(proxies)\n",
    "#proxy = {\"http\":\"http://\" + proxy, \"https\": \"https://\" + proxy}\n",
    "response = requests.get(url, headers=headers, proxies={'http' : proxy,'https': proxy}, timeout=2)\n",
    "response.raise_for_status()\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validaciones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
