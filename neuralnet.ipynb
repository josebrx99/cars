{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import html\n",
    "import time\n",
    "from datetime import datetime\n",
    "from unidecode import unidecode\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd() + \"/src\")\n",
    "import spider\n",
    "import models as models\n",
    "import functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/workspaces/cars/data/df_refine_20240915.parquet\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_numeric = [\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\"]\n",
    "for col in cols_numeric:\n",
    "    df[col] = df[col].astype(float)\n",
    "df_train = df[[\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\", \"transmision\", \"tipo_de_combustible\", \n",
    "                \"tipo_de_carroceria\", \"puertas\",\"marca\", \"modelo_agrup\"]].dropna()\n",
    "df_train[\"precio\"] = np.log(df_train[\"precio\"].astype(float))\n",
    "\n",
    "X_train, X_test, y_train, y_test = models.TrainTestDummies(df = df_train, y='precio',\n",
    "                                                                dummies = [\"marca\", \"modelo_agrup\", \"transmision\", \"tipo_de_combustible\", \n",
    "                                                                        \"tipo_de_carroceria\", \"puertas\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_model(layers, activation='relu', optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    for i, neurons in enumerate(layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(neurons, input_shape=(X_train.shape[1],), activation=activation))\n",
    "        else:\n",
    "            model.add(Dense(neurons, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "scalerX, scalerY = MinMaxScaler(), MinMaxScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_train_scaled = scalerY.fit_transform(y_train)\n",
    "\n",
    "model = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "\n",
    "param_grid = {\n",
    "    'layers': [[32, 64]], \n",
    "    'activation': ['relu'],        \n",
    "    'optimizer': ['adam'],\n",
    "    'epochs': [50, 10],\n",
    "    'batch_size': [20],    \n",
    "}\n",
    "\n",
    "X_new = X_test\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train_scaled, y_train_scaled)\n",
    "print(f\"Best grid: {grid_result.best_params_}\")\n",
    "\n",
    "X_new_scaled = scalerX.transform(X_new)\n",
    "y_pred_scaled = grid_result.best_estimator_.predict(X_new_scaled)\n",
    "y_pred = scalerY.inverse_transform(y_pred_scaled.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(y_pred)\n",
    "y_pred = [float(x) for x in y_pred for item in x]\n",
    "\n",
    "metrics = models.MetricsRegression(np.exp(y_test), X_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/workspaces/cars/data/df_refine_20240915.parquet\")\n",
    "df.shape\n",
    "\n",
    "cols_numeric = [\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\"]\n",
    "for col in cols_numeric:\n",
    "    df[col] = df[col].astype(float)\n",
    "df_train = df[[\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\", \"transmision\", \"tipo_de_combustible\", \n",
    "                \"tipo_de_carroceria\", \"puertas\",\"marca\", \"modelo_agrup\"]].dropna()\n",
    "df_train[\"precio\"] = np.log(df_train[\"precio\"].astype(float))\n",
    "\n",
    "X_train, X_test, y_train, y_test = models.TrainTestDummies(df = df_train, y='precio',\n",
    "                                                                dummies = [\"marca\", \"modelo_agrup\", \"transmision\", \"tipo_de_combustible\", \n",
    "                                                                        \"tipo_de_carroceria\", \"puertas\"])\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyswarm import pso  # Algoritmo de enjambre de partículas\n",
    "\n",
    "X_new = X_test\n",
    "\n",
    "# Función para crear el modelo\n",
    "def create_model(layers, activation='relu'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Añadir las capas ocultas según la lista 'layers'\n",
    "    for i, neurons in enumerate(layers):\n",
    "        if i == 0:  # Primera capa (con input_shape)\n",
    "            model.add(Dense(neurons, input_shape=(X_train.shape[1],), activation=activation))\n",
    "        else:  # Capas ocultas adicionales\n",
    "            model.add(Dense(neurons, activation=activation))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(Dense(1, activation='linear'))  # Salida para regresión\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Función de evaluación (función de costo) para PSO\n",
    "def evaluate_weights(weights, model, X, y):\n",
    "    # Asignar los pesos al modelo\n",
    "    start = 0\n",
    "    for layer in model.layers:\n",
    "        num_weights = np.prod(layer.get_weights()[0].shape)\n",
    "        num_biases = np.prod(layer.get_weights()[1].shape)\n",
    "        layer.set_weights([\n",
    "            weights[start:start + num_weights].reshape(layer.get_weights()[0].shape),\n",
    "            weights[start + num_weights:start + num_weights + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "        ])\n",
    "        start += num_weights + num_biases\n",
    "\n",
    "    # Hacer predicciones con el modelo\n",
    "    predictions = model.predict(X)\n",
    "    \n",
    "    # Calcular la función de costo (error cuadrático medio)\n",
    "    loss = np.mean((predictions - y) ** 2)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Escalar los datos\n",
    "scalerX, scalerY = MinMaxScaler(), MinMaxScaler()\n",
    "X_train_scaled = scalerX.fit_transform(X_train)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "y_train_scaled = scalerY.fit_transform(y_train)\n",
    "\n",
    "# Crear el modelo\n",
    "layers_config = [6, 12]  # Configuración de capas\n",
    "activation_function = 'relu'\n",
    "model = create_model(layers=layers_config, activation=activation_function)\n",
    "\n",
    "# Contar el número total de pesos en el modelo\n",
    "total_weights = sum([np.prod(layer.get_weights()[0].shape) + np.prod(layer.get_weights()[1].shape) for layer in model.layers])\n",
    "\n",
    "# Definir los límites para PSO (mínimo y máximo valor para los pesos)\n",
    "lb = -1 * np.ones(total_weights)\n",
    "ub = 1 * np.ones(total_weights)\n",
    "\n",
    "# Ejecutar PSO para optimizar los pesos del modelo\n",
    "best_weights, best_loss = pso(evaluate_weights, lb, ub, args=(model, X_train_scaled, y_train_scaled), swarmsize=50, maxiter=100)\n",
    "\n",
    "# Asignar los mejores pesos encontrados por PSO al modelo\n",
    "start = 0\n",
    "for layer in model.layers:\n",
    "    num_weights = np.prod(layer.get_weights()[0].shape)\n",
    "    num_biases = np.prod(layer.get_weights()[1].shape)\n",
    "    layer.set_weights([\n",
    "        best_weights[start:start + num_weights].reshape(layer.get_weights()[0].shape),\n",
    "        best_weights[start + num_weights:start + num_weights + num_biases].reshape(layer.get_weights()[1].shape)\n",
    "    ])\n",
    "    start += num_weights + num_biases\n",
    "\n",
    "# Hacer predicciones con el modelo ajustado por PSO\n",
    "X_new_scaled = scalerX.transform(X_new)\n",
    "y_pred_scaled = model.predict(X_new_scaled)\n",
    "y_pred = scalerY.inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(\"Predicciones (escala original):\")\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "año = 2005\n",
    "km = 150000\n",
    "motor = 4\n",
    "marca = \"Ford\"\n",
    "modelo = \"Explorer\"\n",
    "tipo_de_carroceria = \"Camioneta\"\n",
    "puertas = \"4_5\"\n",
    "transmision = \"Automático\"\n",
    "tipo_de_combustible = \"Gasolina\"\n",
    "modelo = df[df[\"modelo\"] == modelo][\"modelo_agrup\"].unique()[0]\n",
    "\n",
    "import joblib\n",
    "files_models = sorted(os.listdir(\"models_pkl\"))[-1]\n",
    "model = joblib.load(\"models_pkl/\" + files_models)  \n",
    "\n",
    "cols_numeric = [\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\"]\n",
    "for col in cols_numeric:\n",
    "    df[col] = df[col].astype(float)\n",
    "df_train = df[[\"precio\", \"año\", \"km\", \"motor\", \"km_por_año\", \"transmision\", \"tipo_de_combustible\", \"tipo_de_carroceria\", \"puertas\",\n",
    "                \"marca\", \"modelo_agrup\"]].dropna()\n",
    "df_train[\"precio\"] = np.log(df_train[\"precio\"].astype(float))\n",
    "\n",
    "dim_models = func.pkl.Load(\"pkl/dim_models_group.pkl\")[[\"modelo\", \"modelo_agrup\"]].drop_duplicates()\n",
    "dim_doors = func.pkl.Load(\"pkl/dim_models_group.pkl\")[[\"modelo\", \"puertas\"]].drop_duplicates()\n",
    "\n",
    "modelo = dim_models[dim_models[\"modelo\"] == modelo][\"modelo_agrup\"].unique()[0]\n",
    "puertas = dim_doors[dim_doors[\"modelo\"] == modelo][\"puertas\"].unique().tolist()\n",
    "\n",
    "if puertas == []:\n",
    "    puertas = \"4_5\"\n",
    "else:\n",
    "    puertas = puertas[0]\n",
    "df_new = pd.DataFrame({\"año\":[año], \"km\":[km], \"motor\":[motor], \"km_por_año\":[km/(datetime.today().year-año)], \n",
    "                        \"transmision\":[transmision], \"tipo_de_combustible\":[tipo_de_combustible],\"marca\":[marca], \n",
    "                        \"modelo_agrup\":[modelo], \"puertas\":[puertas], \"tipo_de_carroceria\":[tipo_de_carroceria]})\n",
    "df_new = pd.get_dummies(df_new, columns = [\"transmision\", \"tipo_de_combustible\", \"marca\", \"modelo_agrup\",\n",
    "                                            \"puertas\", \"tipo_de_carroceria\"]).replace(True, 1)\n",
    "\n",
    "df_predict = pd.DataFrame(columns = model.feature_names_in_)\n",
    "df_predict.loc[0] = 0\n",
    "features = model.feature_names_in_.tolist()\n",
    "features_delete = list(set(df_new.columns) - set(features))\n",
    "if features_delete != []:\n",
    "    df_predict.drop(columns = features_delete, inplace = True)\n",
    "\n",
    "predict = int(np.exp(model.predict(features_delete))*1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
